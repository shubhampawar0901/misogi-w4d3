"""
RAG Chunking Strategy Visualizer
A web application for visualizing different text chunking strategies for RAG systems.
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import PyPDF2
import io
from typing import List, Dict, Tuple, Optional
import time
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import our custom modules
try:
    from chunking_strategies import ChunkingStrategyManager
    from chunk_analyzer import ChunkAnalyzer
    from visualization_engine import VisualizationEngine
    from pdf_processor import PDFProcessor
except ImportError as e:
    st.error(f"Required modules not found: {e}")
    st.stop()

# Page configuration
st.set_page_config(
    page_title="RAG Chunking Visualizer",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #2E86AB;
        text-align: center;
        margin-bottom: 2rem;
    }
    .chunk-card {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #2E86AB;
        margin: 0.5rem 0;
    }
    .strategy-comparison {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .metric-card {
        background-color: #ffffff;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #dee2e6;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
</style>
""", unsafe_allow_html=True)

def initialize_session_state():
    """Initialize session state variables."""
    if 'pdf_text' not in st.session_state:
        st.session_state.pdf_text = ""
    if 'chunking_results' not in st.session_state:
        st.session_state.chunking_results = {}
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = {}

def main():
    """Main application function."""
    initialize_session_state()
    
    # Header
    st.markdown('<h1 class="main-header">ğŸ“š RAG Chunking Strategy Visualizer</h1>', unsafe_allow_html=True)
    st.markdown("**Analyze and visualize different text chunking strategies for RAG systems**")
    
    # Sidebar configuration
    with st.sidebar:
        st.header("âš™ï¸ Configuration")
        
        # Chunking strategies
        st.subheader("ğŸ“ Chunking Strategies")
        
        available_strategies = [
            "Fixed Size",
            "Sentence-based",
            "Paragraph-based", 
            "Semantic",
            "Recursive Character",
            "Token-based",
            "Sliding Window"
        ]
        
        selected_strategies = st.multiselect(
            "Select Strategies to Compare",
            available_strategies,
            default=["Fixed Size", "Sentence-based", "Semantic"],
            help="Choose chunking strategies to analyze and compare"
        )
        
        # Strategy parameters
        st.subheader("ğŸ”§ Parameters")
        
        with st.expander("Fixed Size Parameters"):
            fixed_chunk_size = st.slider("Chunk Size (characters)", 100, 2000, 500)
            fixed_overlap = st.slider("Overlap (characters)", 0, 200, 50)
        
        with st.expander("Sentence-based Parameters"):
            sentences_per_chunk = st.slider("Sentences per Chunk", 1, 10, 3)
            sentence_overlap = st.slider("Sentence Overlap", 0, 3, 1)
        
        with st.expander("Semantic Parameters"):
            semantic_threshold = st.slider("Similarity Threshold", 0.5, 0.95, 0.8)
            min_chunk_size = st.slider("Min Chunk Size", 50, 500, 100)
        
        with st.expander("Token-based Parameters"):
            tokens_per_chunk = st.slider("Tokens per Chunk", 50, 1000, 200)
            token_overlap = st.slider("Token Overlap", 0, 100, 20)
        
        # Analysis options
        st.subheader("ğŸ“Š Analysis Options")
        
        analysis_options = st.multiselect(
            "Select Analysis Types",
            ["Chunk Size Distribution", "Semantic Coherence", "Overlap Analysis", "Embedding Visualization"],
            default=["Chunk Size Distribution", "Semantic Coherence"],
            help="Choose types of analysis to perform"
        )
        
        # Visualization options
        embedding_model = st.selectbox(
            "Embedding Model",
            ["all-MiniLM-L6-v2", "all-mpnet-base-v2", "paraphrase-MiniLM-L6-v2"],
            help="Model for semantic analysis"
        )
    
    # Main content area
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ“„ Document Input")
        
        # Input method selection
        input_method = st.radio(
            "Choose input method:",
            ["Upload PDF", "Paste Text", "Load Sample"],
            horizontal=True
        )
        
        if input_method == "Upload PDF":
            uploaded_file = st.file_uploader(
                "Upload a PDF file",
                type="pdf",
                help="Upload a PDF document to analyze chunking strategies"
            )
            
            if uploaded_file is not None:
                with st.spinner("ğŸ“– Processing PDF..."):
                    try:
                        pdf_processor = PDFProcessor()
                        text = pdf_processor.extract_text(uploaded_file)
                        st.session_state.pdf_text = text
                        
                        # Display PDF info
                        st.success(f"âœ… PDF processed successfully!")
                        st.info(f"ğŸ“Š Document length: {len(text):,} characters")
                        
                        # Preview
                        with st.expander("ğŸ“– Document Preview"):
                            st.text_area("Text Preview", text[:1000] + "..." if len(text) > 1000 else text, height=200, disabled=True)
                    
                    except Exception as e:
                        st.error(f"âŒ Error processing PDF: {e}")
        
        elif input_method == "Paste Text":
            text_input = st.text_area(
                "Paste your text here:",
                height=300,
                placeholder="Enter or paste the text you want to analyze...",
                help="Paste any text content for chunking analysis"
            )
            
            if text_input:
                st.session_state.pdf_text = text_input
                st.info(f"ğŸ“Š Text length: {len(text_input):,} characters")
        
        elif input_method == "Load Sample":
            sample_texts = {
                "Academic Paper": """
                Introduction
                
                Machine learning has revolutionized the field of artificial intelligence, enabling computers to learn and make decisions without explicit programming. This paper explores the latest developments in deep learning architectures and their applications in natural language processing.
                
                The rapid advancement of transformer models has led to significant breakthroughs in language understanding and generation. These models, characterized by their attention mechanisms, have demonstrated remarkable performance across various NLP tasks.
                
                Methodology
                
                Our research methodology involves a comprehensive analysis of existing transformer architectures, including BERT, GPT, and T5 models. We evaluate their performance on standard benchmarks and propose novel improvements to enhance their efficiency and accuracy.
                
                The experimental setup includes training on large-scale datasets and fine-tuning for specific downstream tasks. We employ various evaluation metrics to assess model performance and compare results across different architectures.
                
                Results and Discussion
                
                Our experiments demonstrate that the proposed modifications lead to significant improvements in model performance while reducing computational requirements. The results show consistent gains across multiple evaluation metrics and datasets.
                
                The analysis reveals that attention mechanisms play a crucial role in model effectiveness, and our proposed enhancements to these mechanisms contribute to the observed improvements.
                
                Conclusion
                
                This work presents novel contributions to the field of natural language processing through improved transformer architectures. The proposed methods show promise for future research and practical applications in various domains.
                """,
                "Technical Documentation": """
                System Architecture Overview
                
                The RAG (Retrieval-Augmented Generation) system consists of several key components that work together to provide accurate and contextual responses to user queries. This document outlines the architecture, implementation details, and best practices for deploying RAG systems.
                
                Core Components
                
                1. Document Ingestion Pipeline
                The document ingestion pipeline is responsible for processing various document formats, extracting text content, and preparing it for indexing. This component handles PDF files, Word documents, web pages, and other text sources.
                
                2. Text Chunking Module
                The chunking module divides large documents into smaller, manageable pieces that can be effectively processed by embedding models. Different chunking strategies are available, including fixed-size, semantic, and hierarchical approaches.
                
                3. Embedding Generation
                Text chunks are converted into high-dimensional vector representations using pre-trained embedding models. These embeddings capture semantic meaning and enable similarity-based retrieval.
                
                4. Vector Database
                The vector database stores embeddings and provides efficient similarity search capabilities. Popular options include Pinecone, Weaviate, and Chroma, each offering different features and performance characteristics.
                
                5. Retrieval Engine
                The retrieval engine processes user queries, generates query embeddings, and searches the vector database for relevant chunks. It implements various retrieval strategies and ranking algorithms.
                
                6. Generation Module
                The generation module combines retrieved context with user queries to produce coherent and accurate responses using large language models like GPT-4 or Claude.
                
                Implementation Guidelines
                
                When implementing a RAG system, consider the following best practices:
                - Choose appropriate chunking strategies based on document types
                - Optimize embedding models for your specific domain
                - Implement proper error handling and fallback mechanisms
                - Monitor system performance and user satisfaction metrics
                
                Performance Optimization
                
                To optimize RAG system performance:
                - Use efficient vector databases with proper indexing
                - Implement caching mechanisms for frequently accessed content
                - Consider hybrid search approaches combining dense and sparse retrieval
                - Regularly update and maintain the knowledge base
                """,
                "Story/Narrative": """
                The Lost Library
                
                Chapter 1: Discovery
                
                Sarah had always been drawn to old buildings, but the abandoned library on Elm Street held a particular fascination for her. Its Gothic architecture stood in stark contrast to the modern buildings surrounding it, like a relic from another time.
                
                As she pushed open the heavy wooden doors, the musty smell of old books and forgotten stories enveloped her. Dust particles danced in the shafts of sunlight that filtered through the stained glass windows, creating an almost magical atmosphere.
                
                The main hall was vast, with towering bookshelves that reached toward the vaulted ceiling. Most of the books had been removed long ago, but scattered volumes remained, their leather bindings cracked and faded with age.
                
                Chapter 2: The Hidden Room
                
                While exploring the upper floors, Sarah noticed something peculiar about one of the walls. The wood paneling seemed different, newer somehow. Running her fingers along the edges, she discovered a hidden mechanism.
                
                With a soft click, a section of the wall swung inward, revealing a small, secret room. Inside, she found a collection of manuscripts and documents that appeared to be much older than anything else in the library.
                
                The papers were written in various languages and scripts, some of which she couldn't identify. But one document, written in English, caught her attention. It was titled "The Chronicle of Hidden Knowledge" and appeared to be some kind of historical record.
                
                Chapter 3: Revelations
                
                As Sarah read through the chronicle, she realized she had stumbled upon something extraordinary. The document described a secret society of scholars who had preserved forbidden knowledge throughout history.
                
                The library, it seemed, had been more than just a public institution. It had served as a hidden repository for texts that powerful forces had tried to suppress or destroy. The secret room contained the last remnants of this clandestine collection.
                
                Each manuscript told a different story - scientific discoveries that had been buried, historical events that had been covered up, and philosophical ideas that had been deemed too dangerous for public consumption.
                
                Sarah knew she had to share this discovery with the world, but she also understood the responsibility that came with such knowledge. The chronicle warned of those who would stop at nothing to keep these secrets buried.
                
                As she carefully gathered the most important documents, Sarah heard footsteps echoing through the empty library below. Someone else was in the building, and they were coming her way.
                """
            }
            
            selected_sample = st.selectbox("Choose a sample text:", list(sample_texts.keys()))
            
            if st.button("Load Sample Text"):
                st.session_state.pdf_text = sample_texts[selected_sample]
                st.success("âœ… Sample text loaded!")
                st.info(f"ğŸ“Š Text length: {len(sample_texts[selected_sample]):,} characters")
    
    with col2:
        st.header("ğŸ¯ Quick Actions")
        
        # Analysis button
        analyze_button = st.button(
            "ğŸ” Analyze Chunking Strategies",
            type="primary",
            help="Start chunking analysis",
            disabled=not st.session_state.pdf_text or not selected_strategies
        )
        
        # Clear button
        if st.button("ğŸ—‘ï¸ Clear All", help="Clear all data and results"):
            st.session_state.pdf_text = ""
            st.session_state.chunking_results = {}
            st.session_state.analysis_results = {}
            st.rerun()
        
        # Export button
        if st.session_state.chunking_results:
            if st.button("ğŸ“¥ Export Results", help="Export analysis results"):
                export_results()
    
    # Analysis section
    if analyze_button:
        if not st.session_state.pdf_text:
            st.error("Please provide text input first.")
            return
        
        if not selected_strategies:
            st.error("Please select at least one chunking strategy.")
            return
        
        # Perform chunking analysis
        with st.spinner("ğŸ”„ Analyzing chunking strategies..."):
            try:
                # Initialize components
                strategy_manager = ChunkingStrategyManager()
                analyzer = ChunkAnalyzer(embedding_model)
                visualizer = VisualizationEngine()
                
                # Configure strategy parameters
                strategy_params = {
                    "Fixed Size": {"chunk_size": fixed_chunk_size, "overlap": fixed_overlap},
                    "Sentence-based": {"sentences_per_chunk": sentences_per_chunk, "overlap": sentence_overlap},
                    "Semantic": {"threshold": semantic_threshold, "min_chunk_size": min_chunk_size},
                    "Token-based": {"tokens_per_chunk": tokens_per_chunk, "overlap": token_overlap}
                }
                
                # Apply chunking strategies
                chunking_results = {}
                progress_bar = st.progress(0)
                
                for i, strategy in enumerate(selected_strategies):
                    progress_bar.progress((i + 1) / len(selected_strategies))
                    
                    params = strategy_params.get(strategy, {})
                    chunks = strategy_manager.apply_strategy(st.session_state.pdf_text, strategy, params)
                    chunking_results[strategy] = chunks
                
                progress_bar.empty()
                
                # Analyze chunks
                analysis_results = {}
                for strategy, chunks in chunking_results.items():
                    analysis_results[strategy] = analyzer.analyze_chunks(chunks, analysis_options)
                
                # Store results
                st.session_state.chunking_results = chunking_results
                st.session_state.analysis_results = analysis_results
                
                st.success("âœ… Analysis completed successfully!")
                
            except Exception as e:
                st.error(f"âŒ Error during analysis: {str(e)}")
                return
    
    # Display results
    if st.session_state.chunking_results:
        display_results(
            st.session_state.chunking_results,
            st.session_state.analysis_results,
            selected_strategies,
            analysis_options
        )

def display_results(chunking_results: Dict, analysis_results: Dict, strategies: List[str], analysis_options: List[str]):
    """Display chunking analysis results."""
    st.header("ğŸ“Š Analysis Results")
    
    # Strategy comparison overview
    st.subheader("ğŸ“ˆ Strategy Comparison Overview")
    
    # Create comparison metrics
    comparison_data = []
    for strategy in strategies:
        chunks = chunking_results[strategy]
        analysis = analysis_results[strategy]
        
        comparison_data.append({
            'Strategy': strategy,
            'Total Chunks': len(chunks),
            'Avg Chunk Size': f"{np.mean([len(chunk) for chunk in chunks]):.0f}",
            'Min Chunk Size': f"{min(len(chunk) for chunk in chunks)}",
            'Max Chunk Size': f"{max(len(chunk) for chunk in chunks)}",
            'Std Deviation': f"{np.std([len(chunk) for chunk in chunks]):.0f}"
        })
    
    df_comparison = pd.DataFrame(comparison_data)
    st.dataframe(df_comparison, use_container_width=True)
    
    # Detailed analysis tabs
    if len(strategies) > 1:
        tabs = st.tabs([f"ğŸ“‹ {strategy}" for strategy in strategies] + ["ğŸ”„ Comparison"])
        
        # Individual strategy results
        for i, strategy in enumerate(strategies):
            with tabs[i]:
                display_strategy_results(
                    strategy, 
                    chunking_results[strategy], 
                    analysis_results[strategy],
                    analysis_options
                )
        
        # Comparison tab
        with tabs[-1]:
            display_strategy_comparison(chunking_results, analysis_results, strategies, analysis_options)
    else:
        # Single strategy results
        strategy = strategies[0]
        display_strategy_results(
            strategy,
            chunking_results[strategy],
            analysis_results[strategy], 
            analysis_options
        )

def display_strategy_results(strategy: str, chunks: List[str], analysis: Dict, analysis_options: List[str]):
    """Display results for a single chunking strategy."""
    st.subheader(f"ğŸ“‹ {strategy} Strategy Results")
    
    # Basic metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <h3>ğŸ“Š Total Chunks</h3>
            <h2>{len(chunks)}</h2>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        avg_size = np.mean([len(chunk) for chunk in chunks])
        st.markdown(f"""
        <div class="metric-card">
            <h3>ğŸ“ Avg Size</h3>
            <h2>{avg_size:.0f}</h2>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        std_size = np.std([len(chunk) for chunk in chunks])
        st.markdown(f"""
        <div class="metric-card">
            <h3>ğŸ“ˆ Std Dev</h3>
            <h2>{std_size:.0f}</h2>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        if 'semantic_coherence' in analysis:
            coherence = analysis['semantic_coherence']
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ¯ Coherence</h3>
                <h2>{coherence:.2f}</h2>
            </div>
            """, unsafe_allow_html=True)
    
    # Chunk size distribution
    if "Chunk Size Distribution" in analysis_options:
        st.subheader("ğŸ“Š Chunk Size Distribution")
        
        chunk_sizes = [len(chunk) for chunk in chunks]
        fig = px.histogram(
            x=chunk_sizes,
            nbins=20,
            title=f"Chunk Size Distribution - {strategy}",
            labels={'x': 'Chunk Size (characters)', 'y': 'Frequency'}
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # Sample chunks
    st.subheader("ğŸ“ Sample Chunks")
    
    num_samples = min(5, len(chunks))
    sample_indices = np.linspace(0, len(chunks)-1, num_samples, dtype=int)
    
    for i, idx in enumerate(sample_indices):
        with st.expander(f"Chunk {idx+1} ({len(chunks[idx])} characters)"):
            st.text_area(f"Content", chunks[idx], height=150, disabled=True, key=f"{strategy}_chunk_{i}")

def display_strategy_comparison(chunking_results: Dict, analysis_results: Dict, strategies: List[str], analysis_options: List[str]):
    """Display comparison between different chunking strategies."""
    st.subheader("ğŸ”„ Strategy Performance Comparison")
    
    # Chunk count comparison
    fig = go.Figure()
    
    strategy_names = list(strategies)
    chunk_counts = [len(chunking_results[strategy]) for strategy in strategy_names]
    
    fig.add_trace(go.Bar(
        x=strategy_names,
        y=chunk_counts,
        name='Chunk Count',
        marker_color='lightblue'
    ))
    
    fig.update_layout(
        title="Number of Chunks by Strategy",
        xaxis_title="Chunking Strategy",
        yaxis_title="Number of Chunks"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Size distribution comparison
    if "Chunk Size Distribution" in analysis_options:
        st.subheader("ğŸ“Š Size Distribution Comparison")
        
        fig = go.Figure()
        
        for strategy in strategies:
            chunks = chunking_results[strategy]
            chunk_sizes = [len(chunk) for chunk in chunks]
            
            fig.add_trace(go.Box(
                y=chunk_sizes,
                name=strategy,
                boxpoints='outliers'
            ))
        
        fig.update_layout(
            title="Chunk Size Distribution Comparison",
            yaxis_title="Chunk Size (characters)"
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    # Recommendations
    st.subheader("ğŸ’¡ Strategy Recommendations")
    
    # Analyze results and provide recommendations
    best_for_consistency = min(strategies, key=lambda s: np.std([len(chunk) for chunk in chunking_results[s]]))
    best_for_size = min(strategies, key=lambda s: abs(np.mean([len(chunk) for chunk in chunking_results[s]]) - 500))
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.info(f"""
        **ğŸ¯ Most Consistent**
        
        **{best_for_consistency}** produces the most consistent chunk sizes.
        
        Best for: Predictable processing, uniform retrieval performance.
        """)
    
    with col2:
        st.info(f"""
        **âš–ï¸ Best Size Balance**
        
        **{best_for_size}** produces chunks closest to optimal size (~500 chars).
        
        Best for: General RAG applications, balanced context windows.
        """)

def export_results():
    """Export analysis results."""
    # This would implement export functionality
    st.success("ğŸ“¥ Results exported successfully!")

if __name__ == "__main__":
    main()
